{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fine-tuning Deepspeech  ASR Model with Accent Data.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnUKf-AK3b5o",
        "colab_type": "text"
      },
      "source": [
        "## INTSTALLATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t59IpwIV3inz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        },
        "outputId": "16daceac-428b-49d9-eb38-38971307e240"
      },
      "source": [
        "%cd /content/\n",
        "!wget -O git-lfs.tar.gz https://github.com/git-lfs/git-lfs/releases/download/v2.11.0/git-lfs-linux-amd64-v2.11.0.tar.gz\n",
        "!tar xvzf git-lfs.tar.gz\n",
        "!mkdir git-lfs2.11.0 && mv CHANGELOG.md install.sh README.md git-lfs git-lfs2.11.0\n",
        "!./git-lfs2.11.0/install.sh\n",
        "!git lfs clone https://github.com/mozilla/deepspeech.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "--2020-06-19 10:16:21--  https://github.com/git-lfs/git-lfs/releases/download/v2.11.0/git-lfs-linux-amd64-v2.11.0.tar.gz\n",
            "Resolving github.com (github.com)... 192.30.255.113\n",
            "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/13021798/fa85ce00-9147-11ea-9ec4-c204e7a4e6cd?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20200619%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200619T101621Z&X-Amz-Expires=300&X-Amz-Signature=a2f29f7d963a43bb3e55984102e812c6415ad31d265bda14a14362cca14bc3c9&X-Amz-SignedHeaders=host&actor_id=0&repo_id=13021798&response-content-disposition=attachment%3B%20filename%3Dgit-lfs-linux-amd64-v2.11.0.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2020-06-19 10:16:21--  https://github-production-release-asset-2e65be.s3.amazonaws.com/13021798/fa85ce00-9147-11ea-9ec4-c204e7a4e6cd?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20200619%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200619T101621Z&X-Amz-Expires=300&X-Amz-Signature=a2f29f7d963a43bb3e55984102e812c6415ad31d265bda14a14362cca14bc3c9&X-Amz-SignedHeaders=host&actor_id=0&repo_id=13021798&response-content-disposition=attachment%3B%20filename%3Dgit-lfs-linux-amd64-v2.11.0.tar.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.130.3\n",
            "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.130.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4666614 (4.5M) [application/octet-stream]\n",
            "Saving to: ‘git-lfs.tar.gz’\n",
            "\n",
            "git-lfs.tar.gz      100%[===================>]   4.45M  9.00MB/s    in 0.5s    \n",
            "\n",
            "2020-06-19 10:16:22 (9.00 MB/s) - ‘git-lfs.tar.gz’ saved [4666614/4666614]\n",
            "\n",
            "README.md\n",
            "CHANGELOG.md\n",
            "git-lfs\n",
            "install.sh\n",
            "Git LFS initialized.\n",
            "WARNING: 'git lfs clone' is deprecated and will not be updated\n",
            "          with new flags from 'git clone'\n",
            "\n",
            "'git clone' has been updated in upstream Git to have comparable\n",
            "speeds to 'git lfs clone'.\n",
            "Cloning into 'deepspeech'...\n",
            "remote: Enumerating objects: 77, done.\u001b[K\n",
            "remote: Counting objects: 100% (77/77), done.\u001b[K\n",
            "remote: Compressing objects: 100% (57/57), done.\u001b[K\n",
            "remote: Total 19136 (delta 39), reused 41 (delta 17), pack-reused 19059\u001b[K\n",
            "Receiving objects: 100% (19136/19136), 47.85 MiB | 24.48 MiB/s, done.\n",
            "Resolving deltas: 100% (13056/13056), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvuX8z0O3pgN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.6/site-packages/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icF_jz8a3unp",
        "colab_type": "text"
      },
      "source": [
        "**Don't restart runtime after this, go ahead it will likely work**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLvRgmU53rxh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7f9d6baa-11f4-4b6e-bd00-debf94e37eec"
      },
      "source": [
        "%cd /content/deepspeech/\n",
        "!pip3 install --upgrade pip==20.0.2 wheel==0.34.2 setuptools==46.1.3 \n",
        "!pip3 install --upgrade -e ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/deepspeech\n",
            "Collecting pip==20.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/0c/d01aa759fdc501a58f431eb594a17495f15b88da142ce14b5845662c13f3/pip-20.0.2-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 4.8MB/s \n",
            "\u001b[?25hRequirement already up-to-date: wheel==0.34.2 in /usr/local/lib/python3.6/dist-packages (0.34.2)\n",
            "Collecting setuptools==46.1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/df/635cdb901ee4a8a42ec68e480c49f85f4c59e8816effbf57d9e6ee8b3588/setuptools-46.1.3-py3-none-any.whl (582kB)\n",
            "\u001b[K     |████████████████████████████████| 583kB 20.5MB/s \n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip, setuptools\n",
            "  Found existing installation: pip 19.3.1\n",
            "    Uninstalling pip-19.3.1:\n",
            "      Successfully uninstalled pip-19.3.1\n",
            "  Found existing installation: setuptools 47.3.1\n",
            "    Uninstalling setuptools-47.3.1:\n",
            "      Successfully uninstalled setuptools-47.3.1\n",
            "Successfully installed pip-20.0.2 setuptools-46.1.3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pkg_resources"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Obtaining file:///content/deepspeech\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from deepspeech-training==0.7.4) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: progressbar2 in /usr/local/lib/python3.6/dist-packages (from deepspeech-training==0.7.4) (3.38.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from deepspeech-training==0.7.4) (1.12.0)\n",
            "Collecting pyxdg\n",
            "  Downloading pyxdg-0.26-py2.py3-none-any.whl (40 kB)\n",
            "\u001b[K     |████████████████████████████████| 40 kB 2.6 MB/s \n",
            "\u001b[?25hCollecting attrdict\n",
            "  Downloading attrdict-2.0.1-py2.py3-none-any.whl (9.9 kB)\n",
            "Requirement already satisfied, skipping upgrade: absl-py in /usr/local/lib/python3.6/dist-packages (from deepspeech-training==0.7.4) (0.9.0)\n",
            "Collecting semver\n",
            "  Downloading semver-2.10.2-py2.py3-none-any.whl (12 kB)\n",
            "Collecting opuslib==2.0.0\n",
            "  Downloading opuslib-2.0.0.tar.gz (7.3 kB)\n",
            "Collecting optuna\n",
            "  Downloading optuna-1.5.0.tar.gz (200 kB)\n",
            "\u001b[K     |████████████████████████████████| 200 kB 11.1 MB/s \n",
            "\u001b[?25hCollecting sox\n",
            "  Downloading sox-1.3.7-py2.py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied, skipping upgrade: bs4 in /usr/local/lib/python3.6/dist-packages (from deepspeech-training==0.7.4) (0.0.1)\n",
            "Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.6/dist-packages (from deepspeech-training==0.7.4) (1.0.4)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from deepspeech-training==0.7.4) (2.23.0)\n",
            "Collecting numba==0.47.0\n",
            "  Downloading numba-0.47.0-cp36-cp36m-manylinux1_x86_64.whl (3.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7 MB 15.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: llvmlite==0.31.0 in /usr/local/lib/python3.6/dist-packages (from deepspeech-training==0.7.4) (0.31.0)\n",
            "Requirement already satisfied, skipping upgrade: librosa in /usr/local/lib/python3.6/dist-packages (from deepspeech-training==0.7.4) (0.6.3)\n",
            "Collecting soundfile\n",
            "  Downloading SoundFile-0.10.3.post1-py2.py3-none-any.whl (21 kB)\n",
            "Collecting ds_ctcdecoder==0.7.4\n",
            "  Downloading ds_ctcdecoder-0.7.4-cp36-cp36m-manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 78.8 MB/s \n",
            "\u001b[?25hCollecting tensorflow==1.15.2\n",
            "  Downloading tensorflow-1.15.2-cp36-cp36m-manylinux2010_x86_64.whl (110.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 110.5 MB 20 kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: python-utils>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from progressbar2->deepspeech-training==0.7.4) (2.4.0)\n",
            "Collecting alembic\n",
            "  Downloading alembic-1.4.2.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 63.0 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting cliff\n",
            "  Downloading cliff-3.3.0-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 11.4 MB/s \n",
            "\u001b[?25hCollecting cmaes>=0.5.0\n",
            "  Downloading cmaes-0.5.0-py3-none-any.whl (13 kB)\n",
            "Collecting colorlog\n",
            "  Downloading colorlog-4.1.0-py2.py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.6/dist-packages (from optuna->deepspeech-training==0.7.4) (0.15.1)\n",
            "Requirement already satisfied, skipping upgrade: scipy!=1.4.0 in /usr/local/lib/python3.6/dist-packages (from optuna->deepspeech-training==0.7.4) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: sqlalchemy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from optuna->deepspeech-training==0.7.4) (1.3.17)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from optuna->deepspeech-training==0.7.4) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from bs4->deepspeech-training==0.7.4) (4.6.3)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->deepspeech-training==0.7.4) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->deepspeech-training==0.7.4) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->deepspeech-training==0.7.4) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->deepspeech-training==0.7.4) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->deepspeech-training==0.7.4) (2020.4.5.2)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->deepspeech-training==0.7.4) (2.9)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from numba==0.47.0->deepspeech-training==0.7.4) (46.1.3)\n",
            "Requirement already satisfied, skipping upgrade: audioread>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa->deepspeech-training==0.7.4) (2.1.8)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from librosa->deepspeech-training==0.7.4) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: decorator>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa->deepspeech-training==0.7.4) (4.4.2)\n",
            "Requirement already satisfied, skipping upgrade: resampy>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from librosa->deepspeech-training==0.7.4) (0.2.2)\n",
            "Requirement already satisfied, skipping upgrade: cffi>=1.0 in /usr/local/lib/python3.6/dist-packages (from soundfile->deepspeech-training==0.7.4) (1.14.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 62.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2->deepspeech-training==0.7.4) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2->deepspeech-training==0.7.4) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2->deepspeech-training==0.7.4) (3.10.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 68.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2->deepspeech-training==0.7.4) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2->deepspeech-training==0.7.4) (1.29.0)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2->deepspeech-training==0.7.4) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2->deepspeech-training==0.7.4) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2->deepspeech-training==0.7.4) (0.34.2)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2->deepspeech-training==0.7.4) (3.2.1)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2->deepspeech-training==0.7.4) (1.1.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.1.3-py2.py3-none-any.whl (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 4.5 MB/s \n",
            "\u001b[?25hCollecting python-editor>=0.3\n",
            "  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\n",
            "Collecting cmd2!=0.8.3,>=0.8.0\n",
            "  Downloading cmd2-1.1.0-py3-none-any.whl (120 kB)\n",
            "\u001b[K     |████████████████████████████████| 120 kB 74.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: PrettyTable<0.8,>=0.7.2 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna->deepspeech-training==0.7.4) (0.7.2)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna->deepspeech-training==0.7.4) (2.4.7)\n",
            "Collecting stevedore>=1.20.0\n",
            "  Downloading stevedore-2.0.0-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.7 MB/s \n",
            "\u001b[?25hCollecting pbr!=2.1.0,>=2.0.0\n",
            "  Downloading pbr-5.4.5-py2.py3-none-any.whl (110 kB)\n",
            "\u001b[K     |████████████████████████████████| 110 kB 74.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: PyYAML>=3.12 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna->deepspeech-training==0.7.4) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0->soundfile->deepspeech-training==0.7.4) (2.20)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.2->deepspeech-training==0.7.4) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2->deepspeech-training==0.7.4) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2->deepspeech-training==0.7.4) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from Mako->alembic->optuna->deepspeech-training==0.7.4) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: attrs>=16.3.0 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna->deepspeech-training==0.7.4) (19.3.0)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading pyperclip-1.8.0.tar.gz (16 kB)\n",
            "Requirement already satisfied, skipping upgrade: wcwidth>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna->deepspeech-training==0.7.4) (0.2.4)\n",
            "Collecting colorama>=0.3.7\n",
            "  Downloading colorama-0.4.3-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2->deepspeech-training==0.7.4) (1.6.1)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2->deepspeech-training==0.7.4) (3.1.0)\n",
            "Building wheels for collected packages: opuslib, optuna, alembic, gast, pyperclip\n",
            "  Building wheel for opuslib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for opuslib: filename=opuslib-2.0.0-py3-none-any.whl size=11009 sha256=b19401fe2448e6794ebb743f9fc3b4b24d1367084e7e549285c9f8ac2bac2ede\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/01/88/37797e9e9d157a33eefed22a46aa0bf5044effcec6a9181e41\n",
            "  Building wheel for optuna (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for optuna: filename=optuna-1.5.0-py3-none-any.whl size=276145 sha256=d8a0a1709e6520fca1ece78dc33bae99b761b0c4aada3d1ce1935d8d8029ba47\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/27/3d/f8541909296d8618d30d0da35463d98a84d89b60a167b255f4\n",
            "  Building wheel for alembic (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for alembic: filename=alembic-1.4.2-py2.py3-none-any.whl size=159543 sha256=6080f57df9182cacfcefe3987024de1ff81cfcaca7f81c2eada0d52fde2232a2\n",
            "  Stored in directory: /root/.cache/pip/wheels/f2/50/61/5cc491b0ca39be60dfb4dce940b389ff91b847d62e0eb2d680\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7539 sha256=60829d555eac19508b8ca06360a5c80ddc838e83448f88488b56e366045f12ae\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/a7/b9/0740c7a3a7d1d348f04823339274b90de25fbcd217b2ee1fbe\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.0-py3-none-any.whl size=8691 sha256=7317ef627a4a51e6c16e3628d18198d2d89d35b16be7249b7fe9f8c3e18e59ed\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/30/fe/92e2d4b1301ba74c07ea09c9e4c08f5bf12bae9c30319d74c5\n",
            "Successfully built opuslib optuna alembic gast pyperclip\n",
            "\u001b[31mERROR: umap-learn 0.4.4 has requirement numba!=0.47,>=0.46, but you'll have numba 0.47.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.10.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pyxdg, attrdict, semver, opuslib, Mako, python-editor, alembic, pyperclip, colorama, cmd2, pbr, stevedore, cliff, cmaes, colorlog, optuna, sox, numba, soundfile, ds-ctcdecoder, tensorflow-estimator, tensorboard, gast, tensorflow, deepspeech-training\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.48.0\n",
            "    Uninstalling numba-0.48.0:\n",
            "      Successfully uninstalled numba-0.48.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.2.0\n",
            "    Uninstalling tensorflow-estimator-2.2.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.2.2\n",
            "    Uninstalling tensorboard-2.2.2:\n",
            "      Successfully uninstalled tensorboard-2.2.2\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.2.0\n",
            "    Uninstalling tensorflow-2.2.0:\n",
            "      Successfully uninstalled tensorflow-2.2.0\n",
            "  Running setup.py develop for deepspeech-training\n",
            "Successfully installed Mako-1.1.3 alembic-1.4.2 attrdict-2.0.1 cliff-3.3.0 cmaes-0.5.0 cmd2-1.1.0 colorama-0.4.3 colorlog-4.1.0 deepspeech-training ds-ctcdecoder-0.7.4 gast-0.2.2 numba-0.47.0 optuna-1.5.0 opuslib-2.0.0 pbr-5.4.5 pyperclip-1.8.0 python-editor-1.0.4 pyxdg-0.26 semver-2.10.2 soundfile-0.10.3.post1 sox-1.3.7 stevedore-2.0.0 tensorboard-1.15.0 tensorflow-1.15.2 tensorflow-estimator-1.15.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXiO601V3802",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "outputId": "ab2b35df-e2d7-405c-8833-f7052896f5c1"
      },
      "source": [
        "!pip3 uninstall -y tensorflow\n",
        "!pip3 install 'tensorflow-gpu==1.15.2'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found existing installation: tensorflow 1.15.2\n",
            "Uninstalling tensorflow-1.15.2:\n",
            "  Successfully uninstalled tensorflow-1.15.2\n",
            "Collecting tensorflow-gpu==1.15.2\n",
            "  Downloading tensorflow_gpu-1.15.2-cp36-cp36m-manylinux2010_x86_64.whl (411.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 411.0 MB 35 kB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.2) (0.2.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.2) (1.12.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.2) (1.29.0)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.2) (1.15.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.2) (3.10.0)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.2) (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.2) (1.12.1)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.2) (0.2.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.2) (1.1.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.2) (0.8.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.2) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.2) (1.0.8)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.2) (1.18.5)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.2) (0.9.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.2) (0.34.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.2) (3.2.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.15.2) (46.1.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2) (1.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15.2) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2) (1.6.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2) (3.1.0)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "Successfully installed tensorflow-gpu-1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tqTDqpy4Dyt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "b3e9bc6b-689e-41cf-b14d-1b8241f1cee2"
      },
      "source": [
        "!pip3 install deepspeech-gpu"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting deepspeech-gpu\n",
            "  Downloading deepspeech_gpu-0.7.4-cp36-cp36m-manylinux1_x86_64.whl (19.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.2 MB 4.2 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from deepspeech-gpu) (1.18.5)\n",
            "Installing collected packages: deepspeech-gpu\n",
            "Successfully installed deepspeech-gpu-0.7.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LyVMzq54IjZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a9c8423e-ddca-40d1-c815-3f1d3a2c672b"
      },
      "source": [
        "%cd /content\n",
        "!apt-get install ffmpeg\n",
        "!pip install pydub\n",
        "!apt-get install sox libsox-fmt-mp3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:3.4.6-0ubuntu0.18.04.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 59 not upgraded.\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.24.1-py2.py3-none-any.whl (30 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.24.1\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  libid3tag0 libmad0 libmagic-mgc libmagic1 libopencore-amrnb0\n",
            "  libopencore-amrwb0 libsox-fmt-alsa libsox-fmt-base libsox3\n",
            "Suggested packages:\n",
            "  file libsox-fmt-all\n",
            "The following NEW packages will be installed:\n",
            "  libid3tag0 libmad0 libmagic-mgc libmagic1 libopencore-amrnb0\n",
            "  libopencore-amrwb0 libsox-fmt-alsa libsox-fmt-base libsox-fmt-mp3 libsox3\n",
            "  sox\n",
            "0 upgraded, 11 newly installed, 0 to remove and 59 not upgraded.\n",
            "Need to get 872 kB of archives.\n",
            "After this operation, 7,087 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libopencore-amrnb0 amd64 0.1.3-2.1 [92.0 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libopencore-amrwb0 amd64 0.1.3-2.1 [45.8 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic-mgc amd64 1:5.32-2ubuntu0.4 [184 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic1 amd64 1:5.32-2ubuntu0.4 [68.6 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libid3tag0 amd64 0.15.1b-13 [31.2 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libmad0 amd64 0.15.1b-9ubuntu18.04.1 [64.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libsox3 amd64 14.4.2-3ubuntu0.18.04.1 [226 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libsox-fmt-alsa amd64 14.4.2-3ubuntu0.18.04.1 [10.6 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libsox-fmt-base amd64 14.4.2-3ubuntu0.18.04.1 [32.1 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libsox-fmt-mp3 amd64 14.4.2-3ubuntu0.18.04.1 [15.9 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 sox amd64 14.4.2-3ubuntu0.18.04.1 [101 kB]\n",
            "Fetched 872 kB in 1s (661 kB/s)\n",
            "Selecting previously unselected package libopencore-amrnb0:amd64.\n",
            "(Reading database ... 144328 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libopencore-amrnb0_0.1.3-2.1_amd64.deb ...\n",
            "Unpacking libopencore-amrnb0:amd64 (0.1.3-2.1) ...\n",
            "Selecting previously unselected package libopencore-amrwb0:amd64.\n",
            "Preparing to unpack .../01-libopencore-amrwb0_0.1.3-2.1_amd64.deb ...\n",
            "Unpacking libopencore-amrwb0:amd64 (0.1.3-2.1) ...\n",
            "Selecting previously unselected package libmagic-mgc.\n",
            "Preparing to unpack .../02-libmagic-mgc_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking libmagic-mgc (1:5.32-2ubuntu0.4) ...\n",
            "Selecting previously unselected package libmagic1:amd64.\n",
            "Preparing to unpack .../03-libmagic1_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\n",
            "Selecting previously unselected package libid3tag0:amd64.\n",
            "Preparing to unpack .../04-libid3tag0_0.15.1b-13_amd64.deb ...\n",
            "Unpacking libid3tag0:amd64 (0.15.1b-13) ...\n",
            "Selecting previously unselected package libmad0:amd64.\n",
            "Preparing to unpack .../05-libmad0_0.15.1b-9ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking libmad0:amd64 (0.15.1b-9ubuntu18.04.1) ...\n",
            "Selecting previously unselected package libsox3:amd64.\n",
            "Preparing to unpack .../06-libsox3_14.4.2-3ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking libsox3:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-alsa:amd64.\n",
            "Preparing to unpack .../07-libsox-fmt-alsa_14.4.2-3ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-alsa:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-base:amd64.\n",
            "Preparing to unpack .../08-libsox-fmt-base_14.4.2-3ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-base:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-mp3:amd64.\n",
            "Preparing to unpack .../09-libsox-fmt-mp3_14.4.2-3ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-mp3:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package sox.\n",
            "Preparing to unpack .../10-sox_14.4.2-3ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking sox (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Setting up libid3tag0:amd64 (0.15.1b-13) ...\n",
            "Setting up libmagic-mgc (1:5.32-2ubuntu0.4) ...\n",
            "Setting up libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\n",
            "Setting up libopencore-amrnb0:amd64 (0.1.3-2.1) ...\n",
            "Setting up libmad0:amd64 (0.15.1b-9ubuntu18.04.1) ...\n",
            "Setting up libopencore-amrwb0:amd64 (0.1.3-2.1) ...\n",
            "Setting up libsox3:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Setting up libsox-fmt-mp3:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Setting up libsox-fmt-base:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Setting up libsox-fmt-alsa:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Setting up sox (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwnUD9et4UwP",
        "colab_type": "text"
      },
      "source": [
        "## IMPORT STUFF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQZFXBGH4XDy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from deepspeech import Model\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import sys\n",
        "\n",
        "from os import path\n",
        "\n",
        "\n",
        "import wave\n",
        "import scipy.io.wavfile as wav\n",
        "from pydub import AudioSegment\n",
        "\n",
        "import csv\n",
        "import string\n",
        "import regex as re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1OQZnXy40oM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "a1a9229a-8814-420c-feb1-50a799dd431d"
      },
      "source": [
        "print(tf.test.is_built_with_gpu_support())\n",
        "print(tf.test.is_built_with_cuda())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PeiUl6a-mY8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "615f8900-28e1-4ba4-824e-ed0028776eba"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Jun 19 10:21:34 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.36.06    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-uhJnnU5A4i",
        "colab_type": "text"
      },
      "source": [
        "## CHECK IF THINGS WORK with `ldc binary`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PhewNNX5ccZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c01830ad-8c9c-4f54-8932-0811f30f7ab6"
      },
      "source": [
        "!pwd\n",
        "%cd /content/deepspeech/\n",
        "!./bin/run-ldc93s1.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "/content/deepspeech\n",
            "+ [ ! -f DeepSpeech.py ]\n",
            "+ [ ! -f data/ldc93s1/ldc93s1.csv ]\n",
            "+ echo Downloading and preprocessing LDC93S1 example data, saving in ./data/ldc93s1.\n",
            "Downloading and preprocessing LDC93S1 example data, saving in ./data/ldc93s1.\n",
            "+ python -u bin/import_ldc93s1.py ./data/ldc93s1\n",
            "No path \"./data/ldc93s1\" - creating ...\n",
            "No archive \"./data/ldc93s1/LDC93S1.wav\" - downloading...\n",
            "Progress |                                                      | N/A% completedNo archive \"./data/ldc93s1/LDC93S1.txt\" - downloading...\n",
            "Progress |######################################################| 100% completed\n",
            "Progress |######################################################| 100% completed\n",
            "+ [ -d  ]\n",
            "+ python -c from xdg import BaseDirectory as xdg; print(xdg.save_data_path(\"deepspeech/ldc93s1\"))\n",
            "+ checkpoint_dir=/root/.local/share/deepspeech/ldc93s1\n",
            "+ export CUDA_VISIBLE_DEVICES=0\n",
            "+ python -u DeepSpeech.py --noshow_progressbar --train_files data/ldc93s1/ldc93s1.csv --test_files data/ldc93s1/ldc93s1.csv --train_batch_size 1 --test_batch_size 1 --n_hidden 100 --epochs 200 --checkpoint_dir /root/.local/share/deepspeech/ldc93s1\n",
            "I0619 10:08:38.076999 140704733919104 utils.py:141] NumExpr defaulting to 2 threads.\n",
            "I Could not find best validating checkpoint.\n",
            "I Could not find most recent checkpoint.\n",
            "I Initializing all variables.\n",
            "I STARTING Optimization\n",
            "I Training epoch 0...\n",
            "I Finished training epoch 0 - loss: 367.066650\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 1...\n",
            "I Finished training epoch 1 - loss: 335.958984\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 2...\n",
            "I Finished training epoch 2 - loss: 305.713654\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 3...\n",
            "I Finished training epoch 3 - loss: 276.875580\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 4...\n",
            "I Finished training epoch 4 - loss: 246.741409\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 5...\n",
            "I Finished training epoch 5 - loss: 223.746811\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 6...\n",
            "I Finished training epoch 6 - loss: 201.275665\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 7...\n",
            "I Finished training epoch 7 - loss: 183.947784\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 8...\n",
            "I Finished training epoch 8 - loss: 173.569046\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 9...\n",
            "I Finished training epoch 9 - loss: 170.314606\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 10...\n",
            "I Finished training epoch 10 - loss: 171.134476\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 11...\n",
            "I Finished training epoch 11 - loss: 172.940887\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 12...\n",
            "I Finished training epoch 12 - loss: 172.322754\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 13...\n",
            "I Finished training epoch 13 - loss: 167.915771\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 14...\n",
            "I Finished training epoch 14 - loss: 163.602722\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 15...\n",
            "I Finished training epoch 15 - loss: 158.262817\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 16...\n",
            "I Finished training epoch 16 - loss: 153.836014\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 17...\n",
            "I Finished training epoch 17 - loss: 151.067047\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 18...\n",
            "I Finished training epoch 18 - loss: 149.761581\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 19...\n",
            "I Finished training epoch 19 - loss: 148.874634\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 20...\n",
            "I Finished training epoch 20 - loss: 148.358032\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 21...\n",
            "I Finished training epoch 21 - loss: 146.951035\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 22...\n",
            "I Finished training epoch 22 - loss: 145.639908\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 23...\n",
            "I Finished training epoch 23 - loss: 144.072357\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 24...\n",
            "I Finished training epoch 24 - loss: 143.343796\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 25...\n",
            "I Finished training epoch 25 - loss: 141.214249\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 26...\n",
            "I Finished training epoch 26 - loss: 140.702286\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 27...\n",
            "I Finished training epoch 27 - loss: 140.455032\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 28...\n",
            "I Finished training epoch 28 - loss: 140.453384\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 29...\n",
            "I Finished training epoch 29 - loss: 140.499435\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 30...\n",
            "I Finished training epoch 30 - loss: 139.665466\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 31...\n",
            "I Finished training epoch 31 - loss: 139.355499\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 32...\n",
            "I Finished training epoch 32 - loss: 138.981842\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 33...\n",
            "I Finished training epoch 33 - loss: 139.124283\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 34...\n",
            "I Finished training epoch 34 - loss: 137.839691\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 35...\n",
            "I Finished training epoch 35 - loss: 138.409378\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 36...\n",
            "I Finished training epoch 36 - loss: 137.484619\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 37...\n",
            "I Finished training epoch 37 - loss: 137.517731\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 38...\n",
            "I Finished training epoch 38 - loss: 137.731766\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 39...\n",
            "I Finished training epoch 39 - loss: 136.840683\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 40...\n",
            "I Finished training epoch 40 - loss: 137.878906\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 41...\n",
            "I Finished training epoch 41 - loss: 137.064331\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 42...\n",
            "I Finished training epoch 42 - loss: 136.703918\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 43...\n",
            "I Finished training epoch 43 - loss: 135.753174\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 44...\n",
            "I Finished training epoch 44 - loss: 136.034271\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 45...\n",
            "I Finished training epoch 45 - loss: 135.113358\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 46...\n",
            "I Finished training epoch 46 - loss: 135.152893\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 47...\n",
            "I Finished training epoch 47 - loss: 135.530899\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 48...\n",
            "I Finished training epoch 48 - loss: 134.461670\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 49...\n",
            "I Finished training epoch 49 - loss: 134.542313\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 50...\n",
            "I Finished training epoch 50 - loss: 134.223999\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 51...\n",
            "I Finished training epoch 51 - loss: 134.033112\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 52...\n",
            "I Finished training epoch 52 - loss: 133.072708\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 53...\n",
            "I Finished training epoch 53 - loss: 132.996948\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 54...\n",
            "I Finished training epoch 54 - loss: 132.538162\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 55...\n",
            "I Finished training epoch 55 - loss: 131.741425\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 56...\n",
            "I Finished training epoch 56 - loss: 132.092178\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 57...\n",
            "I Finished training epoch 57 - loss: 131.122589\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 58...\n",
            "I Finished training epoch 58 - loss: 130.938629\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 59...\n",
            "I Finished training epoch 59 - loss: 129.930771\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 60...\n",
            "I Finished training epoch 60 - loss: 130.281265\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 61...\n",
            "I Finished training epoch 61 - loss: 128.411148\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 62...\n",
            "I Finished training epoch 62 - loss: 128.900940\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 63...\n",
            "I Finished training epoch 63 - loss: 128.971664\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 64...\n",
            "I Finished training epoch 64 - loss: 127.835373\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 65...\n",
            "I Finished training epoch 65 - loss: 127.830002\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 66...\n",
            "I Finished training epoch 66 - loss: 126.800140\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 67...\n",
            "I Finished training epoch 67 - loss: 125.470596\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 68...\n",
            "I Finished training epoch 68 - loss: 125.609070\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 69...\n",
            "I Finished training epoch 69 - loss: 124.484192\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 70...\n",
            "I Finished training epoch 70 - loss: 124.624092\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 71...\n",
            "I Finished training epoch 71 - loss: 124.045532\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 72...\n",
            "I Finished training epoch 72 - loss: 123.049980\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 73...\n",
            "I Finished training epoch 73 - loss: 121.219673\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 74...\n",
            "I Finished training epoch 74 - loss: 119.355011\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 75...\n",
            "I Finished training epoch 75 - loss: 119.938942\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 76...\n",
            "I Finished training epoch 76 - loss: 118.405289\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 77...\n",
            "I Finished training epoch 77 - loss: 117.156082\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 78...\n",
            "I Finished training epoch 78 - loss: 116.864296\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 79...\n",
            "I Finished training epoch 79 - loss: 116.155090\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 80...\n",
            "I Finished training epoch 80 - loss: 115.286377\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 81...\n",
            "I Finished training epoch 81 - loss: 114.339645\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 82...\n",
            "I Finished training epoch 82 - loss: 113.464989\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 83...\n",
            "I Finished training epoch 83 - loss: 111.813255\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 84...\n",
            "I Finished training epoch 84 - loss: 110.868744\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 85...\n",
            "I Finished training epoch 85 - loss: 110.552872\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 86...\n",
            "I Finished training epoch 86 - loss: 109.193108\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 87...\n",
            "I Finished training epoch 87 - loss: 108.316719\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 88...\n",
            "I Finished training epoch 88 - loss: 107.867172\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 89...\n",
            "I Finished training epoch 89 - loss: 106.032883\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 90...\n",
            "I Finished training epoch 90 - loss: 105.428078\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 91...\n",
            "I Finished training epoch 91 - loss: 104.569649\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 92...\n",
            "I Finished training epoch 92 - loss: 103.986748\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 93...\n",
            "I Finished training epoch 93 - loss: 101.902138\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 94...\n",
            "I Finished training epoch 94 - loss: 99.749985\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 95...\n",
            "I Finished training epoch 95 - loss: 99.615532\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 96...\n",
            "I Finished training epoch 96 - loss: 99.688705\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 97...\n",
            "I Finished training epoch 97 - loss: 99.106255\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 98...\n",
            "I Finished training epoch 98 - loss: 97.296143\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 99...\n",
            "I Finished training epoch 99 - loss: 96.477585\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 100...\n",
            "I Finished training epoch 100 - loss: 95.306786\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 101...\n",
            "I Finished training epoch 101 - loss: 97.242432\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 102...\n",
            "I Finished training epoch 102 - loss: 94.117821\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 103...\n",
            "I Finished training epoch 103 - loss: 92.417038\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 104...\n",
            "I Finished training epoch 104 - loss: 92.161949\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 105...\n",
            "I Finished training epoch 105 - loss: 92.676064\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 106...\n",
            "I Finished training epoch 106 - loss: 90.754921\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 107...\n",
            "I Finished training epoch 107 - loss: 88.965912\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 108...\n",
            "I Finished training epoch 108 - loss: 88.925880\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 109...\n",
            "I Finished training epoch 109 - loss: 88.282326\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 110...\n",
            "I Finished training epoch 110 - loss: 87.194664\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 111...\n",
            "I Finished training epoch 111 - loss: 84.535423\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 112...\n",
            "I Finished training epoch 112 - loss: 86.084869\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 113...\n",
            "I Finished training epoch 113 - loss: 82.999390\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 114...\n",
            "I Finished training epoch 114 - loss: 82.448250\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 115...\n",
            "I Finished training epoch 115 - loss: 80.651993\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 116...\n",
            "I Finished training epoch 116 - loss: 80.943222\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 117...\n",
            "I Finished training epoch 117 - loss: 80.372932\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 118...\n",
            "I Finished training epoch 118 - loss: 79.578453\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 119...\n",
            "I Finished training epoch 119 - loss: 79.206650\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 120...\n",
            "I Finished training epoch 120 - loss: 79.912834\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 121...\n",
            "I Finished training epoch 121 - loss: 76.977043\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 122...\n",
            "I Finished training epoch 122 - loss: 77.145859\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 123...\n",
            "I Finished training epoch 123 - loss: 73.925056\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 124...\n",
            "I Finished training epoch 124 - loss: 72.433304\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 125...\n",
            "I Finished training epoch 125 - loss: 74.379784\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 126...\n",
            "I Finished training epoch 126 - loss: 73.068916\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 127...\n",
            "I Finished training epoch 127 - loss: 70.800926\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 128...\n",
            "I Finished training epoch 128 - loss: 70.809563\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 129...\n",
            "I Finished training epoch 129 - loss: 68.091270\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 130...\n",
            "I Finished training epoch 130 - loss: 67.260841\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 131...\n",
            "I Finished training epoch 131 - loss: 67.261047\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 132...\n",
            "I Finished training epoch 132 - loss: 67.450272\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 133...\n",
            "I Finished training epoch 133 - loss: 63.640221\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 134...\n",
            "I Finished training epoch 134 - loss: 63.932510\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 135...\n",
            "I Finished training epoch 135 - loss: 61.590527\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 136...\n",
            "I Finished training epoch 136 - loss: 62.808006\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 137...\n",
            "I Finished training epoch 137 - loss: 61.850742\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 138...\n",
            "I Finished training epoch 138 - loss: 60.077187\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 139...\n",
            "I Finished training epoch 139 - loss: 59.399452\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 140...\n",
            "I Finished training epoch 140 - loss: 59.345032\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 141...\n",
            "I Finished training epoch 141 - loss: 56.214703\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 142...\n",
            "I Finished training epoch 142 - loss: 55.358509\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 143...\n",
            "I Finished training epoch 143 - loss: 52.742172\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 144...\n",
            "I Finished training epoch 144 - loss: 53.405865\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 145...\n",
            "I Finished training epoch 145 - loss: 54.233456\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 146...\n",
            "I Finished training epoch 146 - loss: 51.042843\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 147...\n",
            "I Finished training epoch 147 - loss: 50.431473\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 148...\n",
            "I Finished training epoch 148 - loss: 50.714222\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 149...\n",
            "I Finished training epoch 149 - loss: 48.863911\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 150...\n",
            "I Finished training epoch 150 - loss: 48.809471\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 151...\n",
            "I Finished training epoch 151 - loss: 45.468800\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 152...\n",
            "I Finished training epoch 152 - loss: 44.034203\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 153...\n",
            "I Finished training epoch 153 - loss: 45.049488\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 154...\n",
            "I Finished training epoch 154 - loss: 42.650501\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 155...\n",
            "I Finished training epoch 155 - loss: 40.722427\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 156...\n",
            "I Finished training epoch 156 - loss: 44.157078\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 157...\n",
            "I Finished training epoch 157 - loss: 41.920593\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 158...\n",
            "I Finished training epoch 158 - loss: 41.233215\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 159...\n",
            "I Finished training epoch 159 - loss: 38.066654\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 160...\n",
            "I Finished training epoch 160 - loss: 38.354023\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 161...\n",
            "I Finished training epoch 161 - loss: 37.577839\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 162...\n",
            "I Finished training epoch 162 - loss: 36.801052\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 163...\n",
            "I Finished training epoch 163 - loss: 35.967464\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 164...\n",
            "I Finished training epoch 164 - loss: 34.387058\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 165...\n",
            "I Finished training epoch 165 - loss: 33.115257\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 166...\n",
            "I Finished training epoch 166 - loss: 31.452999\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 167...\n",
            "I Finished training epoch 167 - loss: 30.388447\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 168...\n",
            "I Finished training epoch 168 - loss: 31.626522\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 169...\n",
            "I Finished training epoch 169 - loss: 29.542381\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 170...\n",
            "I Finished training epoch 170 - loss: 31.061031\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 171...\n",
            "I Finished training epoch 171 - loss: 26.563004\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 172...\n",
            "I Finished training epoch 172 - loss: 27.451181\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 173...\n",
            "I Finished training epoch 173 - loss: 29.078745\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 174...\n",
            "I Finished training epoch 174 - loss: 26.607468\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 175...\n",
            "I Finished training epoch 175 - loss: 24.110861\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 176...\n",
            "I Finished training epoch 176 - loss: 23.029322\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 177...\n",
            "I Finished training epoch 177 - loss: 23.237770\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 178...\n",
            "I Finished training epoch 178 - loss: 23.475914\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 179...\n",
            "I Finished training epoch 179 - loss: 23.246403\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 180...\n",
            "I Finished training epoch 180 - loss: 21.324732\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 181...\n",
            "I Finished training epoch 181 - loss: 22.453299\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 182...\n",
            "I Finished training epoch 182 - loss: 21.332914\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 183...\n",
            "I Finished training epoch 183 - loss: 19.283155\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 184...\n",
            "I Finished training epoch 184 - loss: 18.577229\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 185...\n",
            "I Finished training epoch 185 - loss: 19.705563\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 186...\n",
            "I Finished training epoch 186 - loss: 19.701160\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 187...\n",
            "I Finished training epoch 187 - loss: 18.191299\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 188...\n",
            "I Finished training epoch 188 - loss: 17.531759\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 189...\n",
            "I Finished training epoch 189 - loss: 17.422848\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 190...\n",
            "I Finished training epoch 190 - loss: 17.725821\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 191...\n",
            "I Finished training epoch 191 - loss: 17.361725\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 192...\n",
            "I Finished training epoch 192 - loss: 14.585348\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 193...\n",
            "I Finished training epoch 193 - loss: 15.804265\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 194...\n",
            "I Finished training epoch 194 - loss: 13.449326\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 195...\n",
            "I Finished training epoch 195 - loss: 12.793380\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 196...\n",
            "I Finished training epoch 196 - loss: 12.798982\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 197...\n",
            "I Finished training epoch 197 - loss: 13.883844\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 198...\n",
            "I Finished training epoch 198 - loss: 14.955746\n",
            "--------------------------------------------------------------------------------\n",
            "I Training epoch 199...\n",
            "I Finished training epoch 199 - loss: 12.801152\n",
            "--------------------------------------------------------------------------------\n",
            "I FINISHED optimization in 0:01:13.279607\n",
            "I Could not find best validating checkpoint.\n",
            "I Loading most recent checkpoint from /root/.local/share/deepspeech/ldc93s1/train-200\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel\n",
            "I Loading variable from checkpoint: global_step\n",
            "I Loading variable from checkpoint: layer_1/bias\n",
            "I Loading variable from checkpoint: layer_1/weights\n",
            "I Loading variable from checkpoint: layer_2/bias\n",
            "I Loading variable from checkpoint: layer_2/weights\n",
            "I Loading variable from checkpoint: layer_3/bias\n",
            "I Loading variable from checkpoint: layer_3/weights\n",
            "I Loading variable from checkpoint: layer_5/bias\n",
            "I Loading variable from checkpoint: layer_5/weights\n",
            "I Loading variable from checkpoint: layer_6/bias\n",
            "I Loading variable from checkpoint: layer_6/weights\n",
            "Testing model on data/ldc93s1/ldc93s1.csv\n",
            "I Test epoch...\n",
            "Test on data/ldc93s1/ldc93s1.csv - WER: 0.000000, CER: 0.000000, loss: 8.932563\n",
            "--------------------------------------------------------------------------------\n",
            "Best WER: \n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 8.932563\n",
            " - wav: file:///content/deepspeech/data/ldc93s1/LDC93S1.wav\n",
            " - src: \"she had your dark suit in greasy wash water all year\"\n",
            " - res: \"she had your dark suit in greasy wash water all year\"\n",
            "--------------------------------------------------------------------------------\n",
            "Median WER: \n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 8.932563\n",
            " - wav: file:///content/deepspeech/data/ldc93s1/LDC93S1.wav\n",
            " - src: \"she had your dark suit in greasy wash water all year\"\n",
            " - res: \"she had your dark suit in greasy wash water all year\"\n",
            "--------------------------------------------------------------------------------\n",
            "Worst WER: \n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 8.932563\n",
            " - wav: file:///content/deepspeech/data/ldc93s1/LDC93S1.wav\n",
            " - src: \"she had your dark suit in greasy wash water all year\"\n",
            " - res: \"she had your dark suit in greasy wash water all year\"\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUFcohzS6dmd",
        "colab_type": "text"
      },
      "source": [
        "## TRAIN, VALIDATE, TEST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moFmmody6phr",
        "colab_type": "text"
      },
      "source": [
        "### GENERATE FILES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIjFOXMq68Oz",
        "colab_type": "text"
      },
      "source": [
        "**For some weird reason, `import_cv2.py` does not work if there is a train-all file present anywhere in the vicinity**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRoMUM966bTQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /content/deepspeech/\n",
        "!bin/import_cv2.py /content/drive/My\\ Drive/Accent\\ Data/english_accented/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_8cS4R-7Dmq",
        "colab_type": "text"
      },
      "source": [
        "***Remove these files in any other language corpora***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOkW74dG2yHy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "efb203e5-1a23-4864-d20f-753749443272"
      },
      "source": [
        "%ls /content/drive/My\\ Drive/'Accent Data'/english_accented/checkpoint_load/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "alphabet.txt                         train-736562.meta\n",
            "best_dev-732522.data-00000-of-00001  train-736846.data-00000-of-00001\n",
            "best_dev-732522.index                train-736846.index\n",
            "best_dev-732522.meta                 train-736846.meta\n",
            "best_dev-736908.index                train-736908.index\n",
            "best_dev-736908.meta                 train-736908.meta\n",
            "best_dev_checkpoint                  train-737293.index\n",
            "checkpoint                           train-737293.meta\n",
            "flags.txt                            train-737578.index\n",
            "train-736562.index                   train-737578.meta\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYtInIpN7JL8",
        "colab_type": "text"
      },
      "source": [
        "### TRAINIIIII.....IIINNG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8rctjzY7cH7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /content/deepspeech/\n",
        "!./DeepSpeech.py \\\n",
        " --n_hidden 2048 \\\n",
        " --save_checkpoint_dir '/content/drive/My Drive/Accent Data/english_accented/checkpoint_load' \\\n",
        " --load_checkpoint_dir '/content/drive/My Drive/Accent Data/english_accented/checkpoint_load' \\\n",
        " --epochs 4 \\\n",
        " --train_files '/content/drive/My Drive/Accent Data/english_accented/clips/train.csv' \\\n",
        " --dev_files '/content/drive/My Drive/Accent Data/english_accented/clips/dev.csv' \\\n",
        " --test_files '/content/drive/My Drive/Accent Data/english_accented/clips/test.csv' \\\n",
        " --alphabet_config_path /content/deepspeech/data/alphabet.txt\\\n",
        " --export_dir '/content/drive/My Drive/Accent Data/english_accented/checkpoint_load' \\\n",
        " --train_cudnn"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}